{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ff60171-fe64-4723-8585-2feea1cdf69c",
   "metadata": {},
   "source": [
    "ASSIGNMENT:ANOMOLY_DETECTION-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d2fd60-30da-4663-aa50-b334be7f44de",
   "metadata": {},
   "source": [
    "1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634b25b5-e2a9-4f59-9e38-a228fad5cd00",
   "metadata": {},
   "source": [
    "\n",
    "Anomaly detection is the process of identifying patterns or data points that deviate significantly from the expected behavior or norm in a given dataset. The purpose of anomaly detection is to identify rare or unusual events or data points that are significantly different from the majority of the data.\n",
    "\n",
    "Anomalies can be caused by a variety of factors such as errors in data collection or processing, unusual events, system failures, fraud, cyber-attacks, or malicious activities. By detecting anomalies, we can identify potential problems or security threats, prevent fraud, improve data quality, and optimize processes.\n",
    "\n",
    "There are various techniques used for anomaly detection, including statistical methods, machine learning algorithms, clustering, and pattern recognition. The choice of technique depends on the nature of the data and the specific problem being addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b81505b-f22e-4fba-a0c4-600a2800b1e4",
   "metadata": {},
   "source": [
    "2.  What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec7e802-feaf-44a9-89df-14f6f1b8ffc5",
   "metadata": {},
   "source": [
    "Data Quality: The quality of data plays a crucial role in anomaly detection. Incomplete, noisy, or inaccurate data can hinder the detection of anomalies and can result in false positives or false negatives.\n",
    "\n",
    "Class Imbalance: Anomalies are typically rare events in a dataset, and the class distribution is imbalanced, with the majority of data points belonging to the normal class. This class imbalance can affect the performance of anomaly detection algorithms, leading to low recall rates for detecting anomalies.\n",
    "\n",
    "Feature Engineering: Feature engineering involves selecting relevant features or variables from the dataset that can help in identifying anomalies. Identifying relevant features is often a challenging task that requires domain knowledge and expertise.\n",
    "\n",
    "Concept Drift: Concept drift occurs when the statistical properties of the data change over time, making it difficult to detect anomalies using a model trained on historical data.\n",
    "\n",
    "Scalability: Anomaly detection algorithms need to be scalable to handle large volumes of data in real-time or near-real-time scenarios.\n",
    "\n",
    "Explainability: In some applications, it is essential to understand the reasons behind the detected anomalies. However, some anomaly detection techniques, such as deep learning-based methods, lack interpretability, making it difficult to explain the reasons for the detected anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f986a0-4689-4c2d-a620-79eb48641a52",
   "metadata": {},
   "source": [
    "3.  How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5874bf1e-449f-463d-928c-be5f17c6516b",
   "metadata": {},
   "source": [
    "Unsupervised anomaly detection and supervised anomaly detection differ in terms of the availability of labeled data during the training phase.\n",
    "\n",
    "In unsupervised anomaly detection, we do not have labeled data and do not know the exact nature or characteristics of the anomalies we are trying to detect. The goal is to identify patterns or data points that deviate significantly from the expected behavior or norm in the dataset. Unsupervised anomaly detection algorithms typically rely on statistical methods, clustering, or density estimation techniques to identify anomalies.\n",
    "\n",
    "In contrast, supervised anomaly detection algorithms require labeled data during the training phase, where we have examples of both normal and anomalous data points. The algorithm learns the characteristics of the normal data points and uses this knowledge to identify anomalies in new, unseen data points. Supervised anomaly detection algorithms typically use machine learning techniques such as decision trees, support vector machines, or neural networks.\n",
    "\n",
    "The main advantage of unsupervised anomaly detection is that it does not require labeled data, making it more applicable to scenarios where labeled data is scarce or unavailable. However, unsupervised techniques may be less accurate than supervised techniques as they do not have access to labeled data for training. Supervised anomaly detection, on the other hand, can be more accurate but requires labeled data for training, which may be costly or time-consuming to obtain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6faaae-a75a-450f-bdf1-0a7806bd209d",
   "metadata": {},
   "source": [
    "4.  What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6053674f-760d-434e-a39a-b2f78c2f7bf5",
   "metadata": {},
   "source": [
    "\n",
    "There are several categories of anomaly detection algorithms, which can be broadly classified into the following categories:\n",
    "\n",
    "Statistical Methods: Statistical methods are based on statistical models that assume that the data is generated by a particular distribution. Anomalies are detected by identifying data points that deviate significantly from the expected distribution. Examples of statistical methods include Z-score, Gaussian distribution, and Student's t-test.\n",
    "\n",
    "Machine Learning Methods: Machine learning methods use algorithms to learn patterns and relationships in the data, which can be used to identify anomalies. Machine learning-based anomaly detection algorithms can be further categorized into supervised and unsupervised methods.\n",
    "\n",
    "Clustering Methods: Clustering methods are based on the idea of grouping similar data points together. Anomalies are detected as data points that do not belong to any of the clusters or belong to a cluster with a very small number of data points.\n",
    "\n",
    "Density-Based Methods: Density-based methods identify anomalies based on the density of data points. Data points with low density are more likely to be anomalies as they are not similar to the majority of data points.\n",
    "\n",
    "Information-Theoretic Methods: Information-theoretic methods measure the amount of information needed to represent a data point. Anomalies are identified as data points that require significantly more information to represent than other data points.\n",
    "\n",
    "Deep Learning Methods: Deep learning methods use neural networks with multiple layers to learn patterns and relationships in the data. Deep learning-based anomaly detection algorithms can be trained in a supervised or unsupervised manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85174d53-a06c-41de-96aa-77bb331625aa",
   "metadata": {},
   "source": [
    "5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1a7209-a650-48e8-bccc-3e354a3da27c",
   "metadata": {},
   "source": [
    "Normality: Distance-based methods assume that the data follows a normal distribution or a known distribution. The distances between the data points are calculated based on these assumptions, and the anomalies are identified as points that deviate significantly from the expected distribution.\n",
    "\n",
    "Euclidean Distance: Distance-based methods assume that the distance between data points can be calculated using the Euclidean distance metric. The Euclidean distance assumes that the data points lie in a Euclidean space and that the distance between two points is the straight line distance between them.\n",
    "\n",
    "Single Cluster: Distance-based methods assume that the data has a single cluster of normal points. The distance between a data point and the center of the cluster is used to identify anomalies. If a data point is far away from the center of the cluster, it is assumed to be an anomaly.\n",
    "\n",
    "Independence: Distance-based methods assume that the data points are independent of each other. Each data point is treated as a separate entity and is analyzed in isolation.\n",
    "\n",
    "Numerical data: Distance-based methods assume that the data is numerical and can be represented as vectors. The distance between two data points is calculated as the distance between their respective vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bd2d1-8b6e-4a1b-9e6b-48862da7bc2a",
   "metadata": {},
   "source": [
    "6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d1c3e-428c-4efb-a157-077505286aae",
   "metadata": {},
   "source": [
    "The LOF algorithm computes anomaly scores as follows:\n",
    "\n",
    "For each data point, LOF computes its k-distance, which is the distance to its k-th nearest neighbor. This step defines the local neighborhood of the data point.\n",
    "\n",
    "LOF then computes the k-distance of each of the data point's k nearest neighbors, which defines the neighborhood of each neighbor.\n",
    "\n",
    "The reachability distance of the data point to each of its k nearest neighbors is then computed. The reachability distance is defined as the maximum of the k-distance of the data point and the distance between the data point and the neighbor.\n",
    "\n",
    "Finally, the local reachability density (LRD) of the data point is computed as the inverse of the average reachability distance of the data point's k nearest neighbors.\n",
    "\n",
    "The anomaly score of the data point is then computed as the average LRD of its k nearest neighbors divided by its own LRD.\n",
    "\n",
    "A high anomaly score indicates that the data point deviates significantly from the expected behavior of its local neighborhood and is likely to be an anomaly. A low anomaly score indicates that the data point is similar to its local neighborhood and is not likely to be an anomaly.\n",
    "\n",
    "LOF is particularly useful for datasets with complex distributions and varying densities, as it is able to capture the local behavior of the data and identify anomalies based on their deviation from the local neighborhood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67949063-21a5-4adc-be25-59634d920bcf",
   "metadata": {},
   "source": [
    "7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558124eb-3721-4157-b6ea-cf0660bf8a91",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is a tree-based unsupervised anomaly detection method that works by isolating anomalies in the data using a set of decision trees. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "Number of Trees (n_estimators): This parameter determines the number of decision trees to be used in the Isolation Forest algorithm. Increasing the number of trees can lead to better performance, but also increases the computational cost.\n",
    "\n",
    "Subsample Size (max_samples): This parameter determines the size of the subsample used to build each decision tree. A smaller subsample size can lead to faster training but may result in lower accuracy. The default value for max_samples is min(256, n), where n is the size of the input data.\n",
    "\n",
    "Number of Features (max_features): This parameter determines the number of features to be used to split each node in the decision tree. A smaller number of features can lead to faster training but may result in lower accuracy. The default value for max_features is the number of features in the input data.\n",
    "\n",
    "Anomaly Threshold (contamination): This parameter determines the proportion of data points that are expected to be anomalies. The default value is 0.1, which assumes that 10% of the data points are anomalies.\n",
    "\n",
    "Random Seed (random_state): This parameter sets the random seed used by the Isolation Forest algorithm. Setting the random seed ensures that the results are reproducible.\n",
    "\n",
    "The values of these parameters can have a significant impact on the performance of the Isolation Forest algorithm. It is important to carefully tune the parameters for each specific dataset and problem to achieve the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1643704-e2a1-407f-ad5d-15e42863a61e",
   "metadata": {},
   "source": [
    "8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score \n",
    "using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c22a70-4a9d-482c-a9a2-337fef3dc938",
   "metadata": {},
   "source": [
    "\n",
    "If a data point has only 2 neighbors of the same class within a radius of 0.5, and we use KNN with K=10 to compute its anomaly score, the score will be high because the data point is not close to enough other points in the dataset.\n",
    "\n",
    "With KNN and K=10, the anomaly score of a data point is based on the distance to its 10th nearest neighbor. In this case, the data point only has 2 neighbors of the same class within a radius of 0.5, which means that it is not close to many other data points in the dataset. As a result, it is unlikely to have many other neighbors within the top 10 nearest neighbors.\n",
    "\n",
    "Therefore, the distance to its 10th nearest neighbor will likely be quite large, indicating that the data point is an anomaly. This will result in a high anomaly score for the data point when using KNN with K=10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425794dc-f4b1-4538-88b4-bca31dd5559f",
   "metadata": {},
   "source": [
    "9.  Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the \n",
    "anomaly score for a data point that has an average path length of 5.0 compared to the average path \n",
    "length of the trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387a0a2-a6bc-4a8f-ab68-54f878ff68e6",
   "metadata": {},
   "source": [
    "In the Isolation Forest algorithm, the anomaly score of a data point is based on the average path length of the data point in a set of isolation trees. The path length is the number of edges traversed to reach the data point's terminal node in each tree. The anomaly score is computed as the average path length normalized by a factor that depends on the number of trees and the size of the dataset.\n",
    "\n",
    "Given that we have 100 trees and a dataset of 3000 data points, we can assume that each tree is constructed with an average of 30 data points per tree. This is because each tree is constructed by randomly selecting a subset of the dataset, and the default value for the subsampling size is 256. Therefore, the average path length for a data point with an average path length of 5.0 compared to the average path length of the trees can be computed as follows:\n",
    "\n",
    "Compute the average path length of the trees for a single data point:\n",
    "\n",
    "The average height of an isolation tree is bounded by O(log_2(n)), where n is the number of data points in the dataset. Therefore, we can assume that the average path length for a data point in a tree is 2 * log_2(n) / 2 = log_2(n).\n",
    "In this case, the dataset has 3000 data points, so the average path length for a data point in a tree is log_2(3000) = 11.55.\n",
    "Compute the anomaly score for the data point:\n",
    "\n",
    "The anomaly score is computed as the average path length of the data point in all the trees, normalized by a factor that depends on the number of trees and the size of the dataset.\n",
    "The normalization factor for 100 trees and a dataset of 3000 data points is 2 * (30 * log_2(30)) / 3000 = 0.503.\n",
    "Therefore, the anomaly score for the data point with an average path length of 5.0 compared to the average path length of the trees is (5.0 / 11.55) / 0.503 = 0.876.\n",
    "Therefore, the anomaly score for the data point is 0.876."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb60810-3927-4d06-8feb-0265299d1aa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
